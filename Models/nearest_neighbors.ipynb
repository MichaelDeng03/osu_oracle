{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nearest Neighbors Prediction for Beatmap in Top Plays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from ossapi import Ossapi\n",
    "import gensim\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "\n",
    "%load_ext cython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each sentence is a iterator of users top score beatmap_ids in order of pp, limited to top plays only.\n",
    "# Sentences is an iterator of sentence(s).\n",
    "class Corpus:\n",
    "    def __init__(self):\n",
    "        self.conn = sqlite3.connect(\"../data/osu.db\")\n",
    "        self.cursor = self.conn.cursor()\n",
    "        query = \"\"\"CREATE TEMP VIEW user_scores AS SELECT beatmap_id, mods, scores.user_id, pp FROM scores JOIN users on scores.user_id = users.user_id\"\"\"\n",
    "        self.cursor.execute(query)\n",
    "\n",
    "    def __iter__(self):\n",
    "        # Iterate over users\n",
    "        ids = self.cursor.execute(\n",
    "            \"SELECT user_id FROM users ORDER BY user_id ASC\"\n",
    "        ).fetchall()\n",
    "\n",
    "        NF = 1\n",
    "        HD = 8  # Removed only for no HD\n",
    "        SD = 32\n",
    "        # NC = 512\n",
    "        SO = 4096\n",
    "        PF = 16384\n",
    "        SV2 = 536870912\n",
    "        # standard_removed_mods = NF | SD | SO | PF | SV2\n",
    "        noHD_removed_mods = NF | SD | HD | SO | PF | SV2\n",
    "\n",
    "        for id in ids:\n",
    "            id = id[0]\n",
    "            scores = self.cursor.execute(\n",
    "                \"SELECT beatmap_id, mods FROM user_scores WHERE user_id = ? ORDER BY pp DESC LIMIT 100\",\n",
    "                (id,),\n",
    "            ).fetchall()\n",
    "\n",
    "            to_yield = []\n",
    "            for score in scores:\n",
    "                bm_id, mod_enum = score\n",
    "\n",
    "                mod_enum &= ~noHD_removed_mods\n",
    "\n",
    "                to_yield.append(str(bm_id) + \"-\" + str(mod_enum))\n",
    "\n",
    "            yield to_yield\n",
    "\n",
    "        self.conn.close()\n",
    "\n",
    "\n",
    "gen = Corpus()\n",
    "sentences = []\n",
    "for sentence in gen:\n",
    "    sentences.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonitorCallback(CallbackAny2Vec):\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "\n",
    "    def on_epoch_end(\n",
    "        self, model\n",
    "    ):  # word2vec accumulates loss, so we need to subtract the previous loss\n",
    "        loss = model.get_latest_training_loss()\n",
    "        if self.epoch == 0:\n",
    "            print(\"Loss after epoch {}: {}\".format(self.epoch, loss))\n",
    "        else:\n",
    "            print(\n",
    "                \"Loss after epoch {}: {}\".format(\n",
    "                    self.epoch, loss - self.loss_previous_step\n",
    "                )\n",
    "            )\n",
    "        self.epoch += 1\n",
    "        self.loss_previous_step = loss\n",
    "\n",
    "\n",
    "model = gensim.models.Word2Vec(\n",
    "    sentences=sentences,\n",
    "    vector_size=15,\n",
    "    epochs=50,\n",
    "    window=100,\n",
    "    min_count=20,  # Higher for generic scores, lower for low frequency scores (top players/niche players).\n",
    "    workers=16,  # 16 thread computer. dunno if it does anything (idek if i have cython working)\n",
    "    sg=0,  # skip-gram significantly better for low freqency words, cbow slightly better for high frequency. SG unreasonable training time with window 100.\n",
    "    hs=0,  # negative sampling for large datasets (training speed).\n",
    "    compute_loss=True,\n",
    "    callbacks=[MonitorCallback()],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "model_name = \"w2v_model_noHD_15d_50e\"\n",
    "os.mkdir(model_name)\n",
    "model.save(model_name + \"/\" + model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize word2vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE  # final reduction\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def reduce_dimensions(model):\n",
    "    num_dimensions = 2\n",
    "\n",
    "    # Limit points for performance\n",
    "    n_points = 5000\n",
    "    vectors_sub, labels_sub = zip(\n",
    "        *random.sample(list(zip(model.wv.vectors, model.wv.index_to_key)), n_points)\n",
    "    )\n",
    "\n",
    "    vectors = np.asarray(vectors_sub)\n",
    "    labels = np.asarray(labels_sub)  # fixed-width numpy strings\n",
    "\n",
    "    # reduce using t-SNE\n",
    "    tsne = TSNE(n_components=num_dimensions, random_state=0)\n",
    "    vectors = tsne.fit_transform(vectors)\n",
    "\n",
    "    x_vals = [v[0] for v in vectors]\n",
    "    y_vals = [v[1] for v in vectors]\n",
    "    return x_vals, y_vals, labels\n",
    "\n",
    "\n",
    "def plot_with_matplotlib(x_vals, y_vals, labels):\n",
    "    random.seed(0)\n",
    "\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.scatter(x_vals, y_vals)\n",
    "\n",
    "    # Label randomly subsampled 25 data points\n",
    "    indices = list(range(len(labels)))\n",
    "    selected_indices = random.sample(indices, 25)\n",
    "    for i in selected_indices:\n",
    "        plt.annotate(labels[i], (x_vals[i], y_vals[i]))\n",
    "\n",
    "\n",
    "x_vals, y_vals, labels = reduce_dimensions(model)\n",
    "plot_with_matplotlib(x_vals, y_vals, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nearest Neighbors Prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# # word2vec_model = model\n",
    "# # else:\n",
    "#     # word2vec_model = gensim.models.Word2Vec.load(\"word2vec-pp/word2vec-pp\")\n",
    "# model = gensim.models.Word2Vec.load(\"w2v_model/w2v_model\")\n",
    "# model.wv.index_to_key[10000:20000]\n",
    "word2vec_model = model\n",
    "NN = NearestNeighbors(n_neighbors=5, algorithm=\"ball_tree\").fit(\n",
    "    word2vec_model.wv.vectors\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"../\")\n",
    "from data.classes import Score\n",
    "from osu_access_token import client_id, client_secret\n",
    "\n",
    "user_id = \"1009285\"\n",
    "api = Ossapi(client_id, client_secret)\n",
    "top_scores = api.user_scores(user_id, type=\"best\", mode=\"osu\", limit=100)\n",
    "\n",
    "top_scores = [Score(score) for score in top_scores]\n",
    "top_scores.sort(key=lambda x: x.pp, reverse=True)\n",
    "\n",
    "top_scores = [\n",
    "    str(score.beatmap_id) + \"-\" + str(score.mods)\n",
    "    for score in top_scores  # Limit to top 50\n",
    "]\n",
    "\n",
    "top_scores_vec = [\n",
    "    score for score in top_scores if score in word2vec_model.wv.index_to_key\n",
    "]\n",
    "top_scores_vec = [word2vec_model.wv[score] for score in top_scores_vec]\n",
    "\n",
    "neighbor = NN.kneighbors([np.mean(top_scores_vec, axis=0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 5 beatmaps\n",
    "beatmaps = [model.wv.index_to_key[i] for i in neighbor[1][0]]\n",
    "beatmaps = [beatmap for beatmap in beatmaps if beatmap not in top_scores]\n",
    "\n",
    "beatmaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare\n",
    "\n",
    "Select random players at different pps from test set, and compare their top plays to avg pp of scores with predicted beatmap_id and pp combination\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(model, user_id, NN):\n",
    "    \"\"\"\n",
    "    Selects users from the database, and compares their total_pp to average pp of model's recommended beatmpas.\n",
    "    \"\"\"\n",
    "    # total_pp\n",
    "    api = Ossapi(client_id, client_secret)\n",
    "    user = api.user(user_id)\n",
    "    pp = user.statistics.pp\n",
    "\n",
    "    # average of recommended beatmaps\n",
    "    top_scores = api.user_scores(user_id, type=\"best\", mode=\"osu\", limit=100)\n",
    "    top_scores = [Score(score) for score in top_scores]\n",
    "    top_scores.sort(key=lambda x: x.pp, reverse=True)\n",
    "    top_scores = [\n",
    "        str(score.beatmap_id) + \"-\" + str(score.mods) for score in top_scores\n",
    "    ]  # Limit to top 50\n",
    "    top_scores_vec = [\n",
    "        score for score in top_scores if score in word2vec_model.wv.index_to_key\n",
    "    ]\n",
    "    top_scores_vec = [word2vec_model.wv[score] for score in top_scores_vec]\n",
    "    neighbor = NN.kneighbors([np.mean(top_scores_vec, axis=0)])\n",
    "    beatmaps = [model.wv.index_to_key[i] for i in neighbor[1][0]]\n",
    "\n",
    "    conn = sqlite3.connect(\"../data/osu.db\")\n",
    "    cursor = conn.cursor()\n",
    "    query = \"\"\"SELECT pp from score where beatmap_id = ? and mods = ?\"\"\"\n",
    "    cursor.execute(query, (beatmaps[0].split(\"-\")[0], beatmaps[0].split(\"-\")[1]))\n",
    "    all_pp = cursor.fetchall()\n",
    "    all_pp = [pp[0] for pp in all_pp]\n",
    "    conn.close()\n",
    "\n",
    "    print(\"User pp/20: \", pp / 20)\n",
    "    print(\"Avg.: \", sum(all_pp) / len(all_pp), \"Beatmap pp: \", all_pp)\n",
    "\n",
    "\n",
    "user_ids = [\n",
    "    \"13767572\",\n",
    "    \"10073635\",\n",
    "    \"8359561\",\n",
    "    \"2944449\",\n",
    "    \"9987634\",\n",
    "    \"7236907\",\n",
    "    \"12900463\",\n",
    "    \"12363937\",\n",
    "    \"13741560\",\n",
    "    \"28956125\",\n",
    "    \"13859779\",\n",
    "    \"2146481\",\n",
    "    \"24195234\",\n",
    "    \"10577632\",\n",
    "]\n",
    "# Manually randomly chosen user_ids with pp ranging from ~15k to ~2k\n",
    "for user_id in user_ids:\n",
    "    compare(model, user_id, NN)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
